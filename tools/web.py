"""
Web Tools - Optimized web search and content extraction
"""
import json
import time
import requests
from bs4 import BeautifulSoup
from langchain_core.tools import tool
from duckduckgo_search import DDGS
from utils.logger import get_logger

logger = get_logger()

# GÃ¼venilir siteler - Ã¶ncelikli
TRUSTED_DOMAINS = [
    # Kod & GeliÅŸtirici
    "github.com", "gitlab.com", "bitbucket.org",
    "stackoverflow.com", "stackexchange.com", "superuser.com",
    "dev.to", "medium.com", "hashnode.dev", "freecodecamp.org",
    
    # Python
    "docs.python.org", "pypi.org", "realpython.com", "pythonbasics.org",
    "learnpython.org", "python.org", "peps.python.org",
    
    # JavaScript/Web
    "developer.mozilla.org", "javascript.info", "nodejs.org",
    "npmjs.com", "yarnpkg.com", "reactjs.org", "vuejs.org",
    "angular.io", "typescriptlang.org", "nextjs.org",
    
    # Genel Programlama
    "w3schools.com", "geeksforgeeks.org", "tutorialspoint.com",
    "javatpoint.com", "programiz.com", "codecademy.com",
    "hackerrank.com", "leetcode.com", "codewars.com",
    
    # Cloud & DevOps
    "docs.microsoft.com", "learn.microsoft.com", "azure.microsoft.com",
    "cloud.google.com", "firebase.google.com",
    "aws.amazon.com", "docs.aws.amazon.com",
    "digitalocean.com", "heroku.com", "vercel.com", "netlify.com",
    "docker.com", "kubernetes.io", "terraform.io",
    
    # VeritabanÄ±
    "postgresql.org", "mysql.com", "mongodb.com", "redis.io",
    "sqlite.org", "mariadb.org",
    
    # AI/ML
    "huggingface.co", "pytorch.org", "tensorflow.org",
    "scikit-learn.org", "keras.io", "openai.com",
    
    # DiÄŸer FaydalÄ±
    "wikipedia.org", "wikimedia.org", "arxiv.org",
    "readthedocs.io", "gitbook.io", "notion.so"
]

# KaÃ§Ä±nÄ±lacak siteler - spam, reklam dolu
BLOCKED_DOMAINS = [
    "pinterest.com", "facebook.com", "twitter.com", "instagram.com",
    "tiktok.com", "linkedin.com", "quora.com", "reddit.com",
    "youtube.com", "vimeo.com", "dailymotion.com"
]


def _is_trusted(url: str) -> bool:
    """Check if URL is from a trusted domain"""
    return any(domain in url.lower() for domain in TRUSTED_DOMAINS)


def _is_blocked(url: str) -> bool:
    """Check if URL should be blocked"""
    return any(domain in url.lower() for domain in BLOCKED_DOMAINS)


def _clean_text(text: str, max_length: int = 500) -> str:
    """Clean and truncate text"""
    # Remove extra whitespace
    text = ' '.join(text.split())
    # Truncate
    if len(text) > max_length:
        text = text[:max_length] + "..."
    return text


@tool
def web_search(query: str, max_results: int = 5) -> str:
    """
    Web'de arama yapar ve Ã¶zet sonuÃ§lar dÃ¶ndÃ¼rÃ¼r.
    SayfalarÄ± ziyaret ETMEZ, sadece arama sonuÃ§larÄ±nÄ± listeler.
    
    Args:
        query: Arama sorgusu
        max_results: Maksimum sonuÃ§ sayÄ±sÄ± (varsayÄ±lan 5)
    
    Returns:
        Arama sonuÃ§larÄ± Ã¶zeti
    """
    logger.info(f"Web search: {query}")
    
    try:
        with DDGS() as ddgs:
            raw_results = list(ddgs.text(query, max_results=max_results + 5))
        
        if not raw_results:
            return "SonuÃ§ bulunamadÄ±. FarklÄ± anahtar kelimeler deneyin."
        
        # Filter and sort results
        results = []
        for r in raw_results:
            url = r.get('href', '')
            
            # Skip blocked domains
            if _is_blocked(url):
                continue
            
            # Prioritize trusted domains
            is_trusted = _is_trusted(url)
            
            results.append({
                "title": r.get('title', '')[:100],
                "url": url,
                "snippet": _clean_text(r.get('body', ''), 200),
                "trusted": is_trusted
            })
        
        # Sort: trusted first
        results.sort(key=lambda x: (not x['trusted'], x['title']))
        results = results[:max_results]
        
        if not results:
            return "Uygun sonuÃ§ bulunamadÄ±."
        
        # Format output - compact
        output = [f"ğŸ” '{query}' iÃ§in {len(results)} sonuÃ§:\n"]
        
        for i, r in enumerate(results, 1):
            trust_icon = "â­" if r['trusted'] else "â€¢"
            output.append(f"{trust_icon} [{i}] {r['title']}")
            output.append(f"   {r['url']}")
            output.append(f"   {r['snippet']}\n")
        
        logger.info(f"Found {len(results)} results")
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return f"Arama hatasÄ±: {e}"


@tool
def quick_answer(question: str) -> str:
    """
    HÄ±zlÄ± cevap almak iÃ§in kullan. Sayfa ziyaret etmeden
    DuckDuckGo instant answer API kullanÄ±r.
    
    Args:
        question: Soru (Ã¶rn: "Python version", "React nedir")
    
    Returns:
        KÄ±sa cevap veya "Cevap bulunamadÄ±"
    """
    logger.info(f"Quick answer: {question}")
    
    try:
        with DDGS() as ddgs:
            # Try instant answers first
            answers = list(ddgs.answers(question))
            if answers:
                answer = answers[0]
                return f"ğŸ’¡ {answer.get('text', 'Cevap bulunamadÄ±')}"
        
        # Fallback to regular search, just return first snippet
        with DDGS() as ddgs:
            results = list(ddgs.text(question, max_results=1))
            if results:
                return f"ğŸ“ {_clean_text(results[0].get('body', ''), 300)}"
        
        return "Cevap bulunamadÄ±."
        
    except Exception as e:
        logger.error(f"Quick answer failed: {e}")
        return f"Hata: {e}"


@tool
def visit_webpage(url: str) -> str:
    """
    Bir web sayfasÄ±nÄ± ziyaret eder ve iÃ§eriÄŸini Ã§Ä±karÄ±r.
    SADECE gÃ¼venilir kaynaklardan bilgi almak iÃ§in kullan.
    Token tasarrufu iÃ§in iÃ§erik kÄ±saltÄ±lÄ±r.
    
    Args:
        url: Ziyaret edilecek URL
    
    Returns:
        Sayfa iÃ§eriÄŸi (kÄ±saltÄ±lmÄ±ÅŸ)
    """
    logger.info(f"Visiting: {url}")
    
    # Warn if not trusted
    if _is_blocked(url):
        return f"âš ï¸ Bu site ({url}) engellendi. FarklÄ± kaynak kullanÄ±n."
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=8)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Remove unwanted elements
        for tag in soup(["script", "style", "nav", "footer", "header", 
                         "aside", "form", "button", "iframe", "noscript"]):
            tag.decompose()
        
        # Try to find main content
        main_content = (
            soup.find("main") or 
            soup.find("article") or 
            soup.find(class_=["content", "post", "entry", "article-body"]) or
            soup.find("body")
        )
        
        if not main_content:
            main_content = soup
        
        # Extract text
        text = main_content.get_text(separator='\n', strip=True)
        
        # Clean up
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        # Remove very short lines (likely menu items)
        lines = [line for line in lines if len(line) > 20 or line.endswith(':')]
        text = '\n'.join(lines)
        
        # Truncate - daha kÄ±sa limit
        max_length = 3000
        if len(text) > max_length:
            text = text[:max_length] + "\n\n...[KÄ±saltÄ±ldÄ±]"
        
        if not text.strip():
            return "Sayfa iÃ§eriÄŸi Ã§Ä±karÄ±lamadÄ±."
        
        trust_note = "â­ GÃ¼venilir kaynak" if _is_trusted(url) else ""
        return f"{trust_note}\nğŸ“„ {url}\n\n{text}"
        
    except requests.Timeout:
        return f"â±ï¸ Zaman aÅŸÄ±mÄ±: {url}"
    except requests.RequestException as e:
        return f"âŒ BaÄŸlantÄ± hatasÄ±: {e}"
    except Exception as e:
        logger.error(f"Visit failed: {e}")
        return f"âŒ Hata: {e}"


@tool
def search_docs(query: str, site: str = "python") -> str:
    """
    Belirli dokÃ¼mantasyon sitelerinde arama yapar.
    
    Args:
        query: Arama sorgusu
        site: Hedef site - "python", "mdn", "npm", "github"
    
    Returns:
        Arama sonuÃ§larÄ±
    """
    site_map = {
        "python": "site:docs.python.org",
        "mdn": "site:developer.mozilla.org",
        "npm": "site:npmjs.com",
        "github": "site:github.com",
        "stackoverflow": "site:stackoverflow.com",
        "pypi": "site:pypi.org"
    }
    
    site_filter = site_map.get(site.lower(), "")
    full_query = f"{query} {site_filter}".strip()
    
    logger.info(f"Docs search: {full_query}")
    
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(full_query, max_results=3))
        
        if not results:
            return f"'{query}' iÃ§in {site} dokÃ¼manlarÄ±nda sonuÃ§ bulunamadÄ±."
        
        output = [f"ğŸ“š {site.upper()} DokÃ¼manlarÄ± - '{query}':\n"]
        
        for i, r in enumerate(results, 1):
            output.append(f"[{i}] {r.get('title', '')[:80]}")
            output.append(f"    {r.get('href', '')}")
            output.append(f"    {_clean_text(r.get('body', ''), 150)}\n")
        
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Docs search failed: {e}")
        return f"Arama hatasÄ±: {e}"
