"""
Web Tools - Optimized web search and content extraction
"""
import json
import time
import requests
from bs4 import BeautifulSoup
from langchain_core.tools import tool
from duckduckgo_search import DDGS
from utils.logger import get_logger

logger = get_logger()

# G√ºvenilir siteler - √∂ncelikli
TRUSTED_DOMAINS = [
    # Kod & Geli≈ütirici
    "github.com", "gitlab.com", "bitbucket.org",
    "stackoverflow.com", "stackexchange.com", "superuser.com",
    "dev.to", "medium.com", "hashnode.dev", "freecodecamp.org",
    
    # Python
    "docs.python.org", "pypi.org", "realpython.com", "pythonbasics.org",
    "learnpython.org", "python.org", "peps.python.org",
    
    # JavaScript/Web
    "developer.mozilla.org", "javascript.info", "nodejs.org",
    "npmjs.com", "yarnpkg.com", "reactjs.org", "vuejs.org",
    "angular.io", "typescriptlang.org", "nextjs.org",
    
    # Genel Programlama
    "w3schools.com", "geeksforgeeks.org", "tutorialspoint.com",
    "javatpoint.com", "programiz.com", "codecademy.com",
    "hackerrank.com", "leetcode.com", "codewars.com",
    
    # Cloud & DevOps
    "docs.microsoft.com", "learn.microsoft.com", "azure.microsoft.com",
    "cloud.google.com", "firebase.google.com",
    "aws.amazon.com", "docs.aws.amazon.com",
    "digitalocean.com", "heroku.com", "vercel.com", "netlify.com",
    "docker.com", "kubernetes.io", "terraform.io",
    
    # Veritabanƒ±
    "postgresql.org", "mysql.com", "mongodb.com", "redis.io",
    "sqlite.org", "mariadb.org",
    
    # AI/ML
    "huggingface.co", "pytorch.org", "tensorflow.org",
    "scikit-learn.org", "keras.io", "openai.com",
    
    # Diƒüer Faydalƒ±
    "wikipedia.org", "wikimedia.org", "arxiv.org",
    "readthedocs.io", "gitbook.io", "notion.so"
]

# Ka√ßƒ±nƒ±lacak siteler - spam, reklam dolu
BLOCKED_DOMAINS = [
    "pinterest.com", "facebook.com", "twitter.com", "instagram.com",
    "tiktok.com", "linkedin.com", "quora.com", "reddit.com",
    "youtube.com", "vimeo.com", "dailymotion.com"
]


def _is_trusted(url: str) -> bool:
    """Check if URL is from a trusted domain"""
    return any(domain in url.lower() for domain in TRUSTED_DOMAINS)


def _is_blocked(url: str) -> bool:
    """Check if URL should be blocked"""
    return any(domain in url.lower() for domain in BLOCKED_DOMAINS)


def _clean_text(text: str, max_length: int = 500) -> str:
    """Clean and truncate text"""
    # Remove extra whitespace
    text = ' '.join(text.split())
    # Truncate
    if len(text) > max_length:
        text = text[:max_length] + "..."
    return text


@tool
def web_search(query: str, max_results: int = 8) -> str:
    """
    Web'de arama yapar ve √∂zet sonu√ßlar d√∂nd√ºr√ºr.
    Sayfalarƒ± ziyaret ETMEZ, sadece arama sonu√ßlarƒ±nƒ± listeler.
    
    Args:
        query: Arama sorgusu
        max_results: Maksimum sonu√ß sayƒ±sƒ± (varsayƒ±lan 8)
    
    Returns:
        Arama sonu√ßlarƒ± √∂zeti
    """
    logger.info(f"Web search: {query}")
    
    try:
        # DuckDuckGo ile arama - daha fazla sonu√ß al
        with DDGS() as ddgs:
            raw_results = list(ddgs.text(
                query, 
                max_results=max_results * 3,  # Daha fazla al, sonra filtrele
                region='wt-wt',  # Worldwide
                safesearch='moderate'
            ))
        
        if not raw_results:
            # Alternatif: news aramasƒ± dene
            logger.info("Text search empty, trying news...")
            with DDGS() as ddgs:
                raw_results = list(ddgs.news(query, max_results=max_results))
        
        if not raw_results:
            return f"'{query}' i√ßin sonu√ß bulunamadƒ±. Farklƒ± anahtar kelimeler deneyin."
        
        # Filter and sort results
        results = []
        seen_urls = set()
        
        for r in raw_results:
            url = r.get('href', r.get('url', ''))
            
            # Skip duplicates
            if url in seen_urls:
                continue
            seen_urls.add(url)
            
            # Skip blocked domains
            if _is_blocked(url):
                continue
            
            # Skip empty results
            title = r.get('title', '')
            snippet = r.get('body', r.get('description', ''))
            if not title or not snippet:
                continue
            
            # Prioritize trusted domains
            is_trusted = _is_trusted(url)
            
            results.append({
                "title": title[:120],
                "url": url,
                "snippet": _clean_text(snippet, 250),
                "trusted": is_trusted
            })
        
        # Sort: trusted first, then by title length (longer = more descriptive)
        results.sort(key=lambda x: (not x['trusted'], -len(x['snippet'])))
        results = results[:max_results]
        
        if not results:
            return f"'{query}' i√ßin uygun sonu√ß bulunamadƒ±. Sorguyu deƒüi≈ütirmeyi deneyin."
        
        # Format output - daha detaylƒ±
        output = [f"üîç '{query}' i√ßin {len(results)} sonu√ß bulundu:\n"]
        
        for i, r in enumerate(results, 1):
            trust_icon = "‚≠ê" if r['trusted'] else "‚Ä¢"
            output.append(f"{trust_icon} [{i}] {r['title']}")
            output.append(f"   üîó {r['url']}")
            output.append(f"   üìù {r['snippet']}\n")
        
        output.append("\nüí° Detaylƒ± bilgi i√ßin visit_webpage(url) kullanƒ±n.")
        
        logger.info(f"Found {len(results)} results for '{query}'")
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return f"Arama hatasƒ±: {e}. L√ºtfen tekrar deneyin."


@tool
def quick_answer(question: str) -> str:
    """
    Hƒ±zlƒ± cevap almak i√ßin kullan. Sayfa ziyaret etmeden
    DuckDuckGo instant answer API kullanƒ±r.
    
    Args:
        question: Soru (√∂rn: "Python version", "React nedir")
    
    Returns:
        Kƒ±sa cevap veya "Cevap bulunamadƒ±"
    """
    logger.info(f"Quick answer: {question}")
    
    try:
        # Try instant answers first
        with DDGS() as ddgs:
            answers = list(ddgs.answers(question))
            if answers:
                answer = answers[0]
                text = answer.get('text', '')
                url = answer.get('url', '')
                if text:
                    result = f"üí° {text}"
                    if url:
                        result += f"\nüîó {url}"
                    return result
        
        # Try Wikipedia-style search
        wiki_query = f"{question} site:wikipedia.org"
        with DDGS() as ddgs:
            results = list(ddgs.text(wiki_query, max_results=1))
            if results:
                r = results[0]
                return f"üìö {r.get('title', '')}\n{_clean_text(r.get('body', ''), 400)}\nüîó {r.get('href', '')}"
        
        # Fallback to regular search
        with DDGS() as ddgs:
            results = list(ddgs.text(question, max_results=3))
            if results:
                output = []
                for r in results[:2]:
                    output.append(f"üìù {r.get('title', '')}")
                    output.append(f"   {_clean_text(r.get('body', ''), 200)}")
                return "\n".join(output)
        
        return f"'{question}' i√ßin cevap bulunamadƒ±. web_search() ile detaylƒ± arama yapabilirsiniz."
        
    except Exception as e:
        logger.error(f"Quick answer failed: {e}")
        return f"Hata: {e}"


@tool
def visit_webpage(url: str) -> str:
    """
    Bir web sayfasƒ±nƒ± ziyaret eder ve i√ßeriƒüini √ßƒ±karƒ±r.
    SADECE g√ºvenilir kaynaklardan bilgi almak i√ßin kullan.
    Token tasarrufu i√ßin i√ßerik kƒ±saltƒ±lƒ±r.
    
    Args:
        url: Ziyaret edilecek URL
    
    Returns:
        Sayfa i√ßeriƒüi (kƒ±saltƒ±lmƒ±≈ü)
    """
    logger.info(f"Visiting: {url}")
    
    # Warn if not trusted
    if _is_blocked(url):
        return f"‚ö†Ô∏è Bu site ({url}) engellendi. Farklƒ± kaynak kullanƒ±n."
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=8)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Remove unwanted elements
        for tag in soup(["script", "style", "nav", "footer", "header", 
                         "aside", "form", "button", "iframe", "noscript"]):
            tag.decompose()
        
        # Try to find main content
        main_content = (
            soup.find("main") or 
            soup.find("article") or 
            soup.find(class_=["content", "post", "entry", "article-body"]) or
            soup.find("body")
        )
        
        if not main_content:
            main_content = soup
        
        # Extract text
        text = main_content.get_text(separator='\n', strip=True)
        
        # Clean up
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        # Remove very short lines (likely menu items)
        lines = [line for line in lines if len(line) > 20 or line.endswith(':')]
        text = '\n'.join(lines)
        
        # Truncate - daha kƒ±sa limit
        max_length = 3000
        if len(text) > max_length:
            text = text[:max_length] + "\n\n...[Kƒ±saltƒ±ldƒ±]"
        
        if not text.strip():
            return "Sayfa i√ßeriƒüi √ßƒ±karƒ±lamadƒ±."
        
        trust_note = "‚≠ê G√ºvenilir kaynak" if _is_trusted(url) else ""
        return f"{trust_note}\nüìÑ {url}\n\n{text}"
        
    except requests.Timeout:
        return f"‚è±Ô∏è Zaman a≈üƒ±mƒ±: {url}"
    except requests.RequestException as e:
        return f"‚ùå Baƒülantƒ± hatasƒ±: {e}"
    except Exception as e:
        logger.error(f"Visit failed: {e}")
        return f"‚ùå Hata: {e}"


@tool
def search_docs(query: str, site: str = "python") -> str:
    """
    Belirli dok√ºmantasyon sitelerinde arama yapar.
    
    Args:
        query: Arama sorgusu
        site: Hedef site - "python", "mdn", "npm", "github"
    
    Returns:
        Arama sonu√ßlarƒ±
    """
    site_map = {
        "python": "site:docs.python.org",
        "mdn": "site:developer.mozilla.org",
        "npm": "site:npmjs.com",
        "github": "site:github.com",
        "stackoverflow": "site:stackoverflow.com",
        "pypi": "site:pypi.org"
    }
    
    site_filter = site_map.get(site.lower(), "")
    full_query = f"{query} {site_filter}".strip()
    
    logger.info(f"Docs search: {full_query}")
    
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(full_query, max_results=3))
        
        if not results:
            return f"'{query}' i√ßin {site} dok√ºmanlarƒ±nda sonu√ß bulunamadƒ±."
        
        output = [f"üìö {site.upper()} Dok√ºmanlarƒ± - '{query}':\n"]
        
        for i, r in enumerate(results, 1):
            output.append(f"[{i}] {r.get('title', '')[:80]}")
            output.append(f"    {r.get('href', '')}")
            output.append(f"    {_clean_text(r.get('body', ''), 150)}\n")
        
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Docs search failed: {e}")
        return f"Arama hatasƒ±: {e}"


@tool
def get_page_title(url: str) -> str:
    """
    Web sayfasƒ±nƒ±n ba≈ülƒ±ƒüƒ±nƒ± alƒ±r.
    
    Args:
        url: Sayfa URL'i
    
    Returns:
        Sayfa ba≈ülƒ±ƒüƒ± veya hata mesajƒ±
    """
    try:
        if _is_blocked(url):
            return f"‚ö†Ô∏è Bu site ({url}) engellendi."
            
    except Exception as e:
        return f"‚ùå Hata: {str(e)}"


@tool
def browse_site(url: str) -> str:
    """
    Opens a URL in a browser inside the Docker sandbox.
    The browser runs inside VNC so you can see and interact with it.
    
    Args:
        url: URL to visit
        
    Returns:
        Status message
    """
    import subprocess
    from tools.sandbox import _is_container_running, DOCKER_DIR
    
    logger.info(f"Browsing in VNC: {url}")
    
    if not url.startswith('http'):
        url = 'https://' + url
    
    # Check if container is running
    if not _is_container_running():
        return "‚ùå Sandbox is not running. Use sandbox_start first."
    
    try:
        # Start VNC if not already running
        subprocess.run(
            ["docker", "exec", "atomagent-sandbox", "bash", "-c", 
             "pgrep -x Xvfb || /home/agent/start-vnc.sh"],
            capture_output=True, text=True, timeout=15, cwd=DOCKER_DIR
        )
        
        # Open URL in Chromium inside Docker - fullscreen mode
        # Kill existing browser first, then open new one in fullscreen
        subprocess.run(
            ["docker", "exec", "atomagent-sandbox", "bash", "-c", 
             f'pkill -f chromium || true; sleep 0.5; DISPLAY=:99 chromium-browser --no-sandbox --disable-gpu --disable-software-rasterizer --start-fullscreen --start-maximized "{url}" &'],
            capture_output=True, text=True, timeout=10, cwd=DOCKER_DIR
        )
        
        return f"üåê Browser opened at {url} in VNC. (View in Preview tab - VNC mode will activate)"
        
    except subprocess.TimeoutExpired:
        return f"‚ö†Ô∏è Timeout starting browser, but VNC may still work. URL: {url}"
    except Exception as e:
        logger.error(f"browse_site error: {e}")
        return f"‚ùå Error: {str(e)}"

