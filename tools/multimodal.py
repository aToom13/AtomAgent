"""
Multi-Modal Tools - GÃ¶rsel analiz ve ses iÅŸleme
Vision (gÃ¶rÃ¼ntÃ¼ analizi) ve Audio (ses) desteÄŸi
"""
import os
import base64
from typing import Optional
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage

from config import config
from utils.logger import get_logger
from core.providers import model_manager, get_api_key

logger = get_logger()

WORKSPACE_DIR = config.workspace.base_dir


def _encode_image(image_path: str) -> Optional[str]:
    """GÃ¶rÃ¼ntÃ¼yÃ¼ base64'e encode et"""
    try:
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode("utf-8")
    except Exception as e:
        logger.error(f"Image encoding failed: {e}")
        return None


def _get_image_mime_type(image_path: str) -> str:
    """GÃ¶rÃ¼ntÃ¼ MIME tipini belirle"""
    ext = os.path.splitext(image_path)[1].lower()
    mime_types = {
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
        ".gif": "image/gif",
        ".webp": "image/webp",
        ".bmp": "image/bmp"
    }
    return mime_types.get(ext, "image/png")


@tool
def analyze_image(image_path: str, question: str = "Bu gÃ¶rselde ne var? DetaylÄ± aÃ§Ä±kla.") -> str:
    """
    Bir gÃ¶rÃ¼ntÃ¼yÃ¼ analiz eder ve aÃ§Ä±klama dÃ¶ndÃ¼rÃ¼r.
    Vision destekleyen modeller gerektirir (GPT-4V, Claude 3, Gemini Pro Vision).
    
    Args:
        image_path: GÃ¶rÃ¼ntÃ¼ dosyasÄ±nÄ±n yolu (workspace iÃ§inde)
        question: GÃ¶rÃ¼ntÃ¼ hakkÄ±nda sorulacak soru
    
    Returns:
        GÃ¶rÃ¼ntÃ¼ analizi sonucu
    
    Ã–rnek:
        analyze_image("screenshot.png", "Bu ekran gÃ¶rÃ¼ntÃ¼sÃ¼ndeki hata nedir?")
        analyze_image("diagram.png", "Bu diyagramÄ± aÃ§Ä±kla")
    """
    
    logger.info(f"Analyzing image: {image_path}")
    
    # Dosya yolunu kontrol et
    full_path = os.path.join(WORKSPACE_DIR, image_path)
    if not os.path.exists(full_path):
        # Mutlak yol dene
        if os.path.exists(image_path):
            full_path = image_path
        else:
            return f"âŒ GÃ¶rÃ¼ntÃ¼ bulunamadÄ±: {image_path}"
    
    # GÃ¶rÃ¼ntÃ¼yÃ¼ encode et
    image_data = _encode_image(full_path)
    if not image_data:
        return "âŒ GÃ¶rÃ¼ntÃ¼ okunamadÄ±"
    
    mime_type = _get_image_mime_type(full_path)
    
    # Vision destekleyen model al (yeni vision rolÃ¼)
    llm = model_manager.get_llm("vision")
    
    if not llm:
        return "âŒ Vision modeli baÅŸlatÄ±lamadÄ±. LÃ¼tfen :model komutu ile vision ayarlarÄ±nÄ± kontrol edin."
    
    try:
        # Vision mesajÄ± oluÅŸtur
        message = HumanMessage(
            content=[
                {"type": "text", "text": question},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{mime_type};base64,{image_data}"
                    }
                }
            ]
        )
        
        response = llm.invoke([message])
        
        logger.info("Image analysis completed")
        return f"ğŸ–¼ï¸ GÃ¶rÃ¼ntÃ¼ Analizi:\n\n{response.content}"
        
    except Exception as e:
        error_str = str(e).lower()
        
        # Fallback dene
        if model_manager.switch_to_fallback("vision"):
             return analyze_image(image_path, question)

        if "vision" in error_str or "image" in error_str or "multimodal" in error_str:
            return """âŒ Bu model gÃ¶rÃ¼ntÃ¼ analizi desteklemiyor.

Vision destekleyen modeller:
â€¢ OpenAI: gpt-4-vision-preview, gpt-4o
â€¢ Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku
â€¢ Google: gemini-pro-vision, gemini-1.5-pro

:model komutu ile vision destekleyen bir model seÃ§in."""
        
        logger.error(f"Image analysis failed: {e}")
        return f"âŒ Analiz hatasÄ±: {e}"


@tool
def analyze_screenshot(question: str = "Bu ekran gÃ¶rÃ¼ntÃ¼sÃ¼nde ne gÃ¶rÃ¼yorsun?") -> str:
    """
    Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±r ve analiz eder.
    
    Args:
        question: Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ hakkÄ±nda soru
    
    Returns:
        Analiz sonucu
    
    Not: Bu fonksiyon pyautogui veya pillow gerektirir.
    """
    try:
        from PIL import ImageGrab
        import tempfile
        
        # Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ al
        screenshot = ImageGrab.grab()
        
        # GeÃ§ici dosyaya kaydet
        temp_path = os.path.join(WORKSPACE_DIR, "_screenshot_temp.png")
        screenshot.save(temp_path)
        
        # Analiz et
        result = analyze_image.invoke({
            "image_path": temp_path,
            "question": question
        })
        
        # GeÃ§ici dosyayÄ± sil
        try:
            os.remove(temp_path)
        except:
            pass
        
        return result
        
    except ImportError:
        return """âŒ Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ iÃ§in gerekli paketler yÃ¼klÃ¼ deÄŸil.

YÃ¼klemek iÃ§in:
pip install pillow

Linux'ta ayrÄ±ca:
sudo apt install python3-tk python3-dev"""
    except Exception as e:
        logger.error(f"Screenshot failed: {e}")
        return f"âŒ Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ alÄ±namadÄ±: {e}"


@tool
def describe_code_screenshot(image_path: str) -> str:
    """
    Kod iÃ§eren bir ekran gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ analiz eder.
    Hata mesajlarÄ±, kod yapÄ±sÄ± ve sorunlarÄ± tespit eder.
    
    Args:
        image_path: Kod ekran gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼n yolu
    
    Returns:
        Kod analizi ve Ã¶neriler
    """
    question = """Bu ekran gÃ¶rÃ¼ntÃ¼sÃ¼ndeki kodu analiz et:

1. Hangi programlama dili kullanÄ±lmÄ±ÅŸ?
2. Kodun ne yaptÄ±ÄŸÄ±nÄ± aÃ§Ä±kla
3. GÃ¶rÃ¼nen hata mesajlarÄ± varsa aÃ§Ä±kla
4. Potansiyel sorunlar veya iyileÅŸtirme Ã¶nerileri var mÄ±?
5. EÄŸer bir hata varsa, nasÄ±l dÃ¼zeltilebilir?

DetaylÄ± ve teknik bir analiz yap."""
    
    return analyze_image.invoke({
        "image_path": image_path,
        "question": question
    })


@tool
def extract_text_from_image(image_path: str) -> str:
    """
    GÃ¶rÃ¼ntÃ¼den metin Ã§Ä±karÄ±r (OCR).
    
    Args:
        image_path: GÃ¶rÃ¼ntÃ¼ dosyasÄ±nÄ±n yolu
    
    Returns:
        Ã‡Ä±karÄ±lan metin
    """
    question = """Bu gÃ¶rÃ¼ntÃ¼deki TÃœM metni oku ve aynen yaz.
FormatÄ± koru (satÄ±r sonlarÄ±, girintiler).
Sadece metni yaz, yorum ekleme."""
    
    return analyze_image.invoke({
        "image_path": image_path,
        "question": question
    })


@tool
def analyze_diagram(image_path: str) -> str:
    """
    Teknik diyagramÄ± (flowchart, UML, mimari) analiz eder.
    
    Args:
        image_path: Diyagram gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼n yolu
    
    Returns:
        Diyagram aÃ§Ä±klamasÄ± ve analizi
    """
    question = """Bu teknik diyagramÄ± analiz et:

1. Diyagram tipi nedir? (flowchart, UML, ER diagram, mimari, vb.)
2. Ana bileÅŸenleri listele
3. BileÅŸenler arasÄ± iliÅŸkileri aÃ§Ä±kla
4. Veri/kontrol akÄ±ÅŸÄ±nÄ± aÃ§Ä±kla
5. Varsa eksik veya belirsiz noktalarÄ± belirt

Teknik ve detaylÄ± bir analiz yap."""
    
    return analyze_image.invoke({
        "image_path": image_path,
        "question": question
    })


# ============================================
# AUDIO TOOLS (Ses Ä°ÅŸleme)
# ============================================

@tool
def transcribe_audio(audio_path: str) -> str:
    """
    Ses dosyasÄ±nÄ± metne Ã§evirir (Speech-to-Text).
    OpenAI Whisper API veya yerel Whisper modeli kullanÄ±r.
    
    Args:
        audio_path: Ses dosyasÄ±nÄ±n yolu (.mp3, .wav, .m4a, .webm)
    
    Returns:
        Transkript metni
    """
    logger.info(f"Transcribing audio: {audio_path}")
    
    # Dosya yolunu kontrol et
    full_path = os.path.join(WORKSPACE_DIR, audio_path)
    if not os.path.exists(full_path):
        if os.path.exists(audio_path):
            full_path = audio_path
        else:
            return f"âŒ Ses dosyasÄ± bulunamadÄ±: {audio_path}"
    
    # Audio config al
    config = model_manager.get_config("audio")
    provider = config.provider
    
    # OpenAI Whisper API
    if provider == "openai":
        try:
            import openai
            
            api_key = get_api_key("openai")
            if not api_key:
                 return "âŒ OpenAI API key bulunamadÄ±."

            client = openai.OpenAI(api_key=api_key)
            
            with open(full_path, "rb") as audio_file:
                transcript = client.audio.transcriptions.create(
                    model=config.model, # whisper-1
                    file=audio_file
                )
            
            logger.info("Audio transcription completed (OpenAI)")
            return f"ğŸ¤ Transkript:\n\n{transcript.text}"
            
        except Exception as e:
            logger.warning(f"OpenAI Whisper failed: {e}")
            # Fallback
            if model_manager.switch_to_fallback("audio"):
                 return transcribe_audio(audio_path)
            return f"âŒ Transkript hatasÄ±: {e}"
    
    # Yerel Whisper (ollama veya local provider olarak iÅŸaretlenmiÅŸse)
    elif provider == "local" or provider == "ollama":
        try:
            import whisper
            
            model_name = "base" # VarsayÄ±lan
            if config.model and config.model != "whisper-1":
                 model_name = config.model

            model = whisper.load_model(model_name)
            result = model.transcribe(full_path)
            
            logger.info("Audio transcription completed (local Whisper)")
            return f"ğŸ¤ Transkript:\n\n{result['text']}"
            
        except ImportError:
            return """âŒ Ses transkripti iÃ§in gerekli paketler yÃ¼klÃ¼ deÄŸil.

SeÃ§enekler:
1. OpenAI API (Ã¶nerilen):
   - :model audio openai whisper-1

2. Yerel Whisper:
   - pip install openai-whisper
   - Ä°lk kullanÄ±mda model indirilecek (~1GB)"""
        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            return f"âŒ Transkript hatasÄ±: {e}"

    # Hugging Face Inference API
    elif provider == "huggingface":
        try:
            import requests
            
            api_key = get_api_key("huggingface")
            if not api_key:
                 return "âŒ Hugging Face API key bulunamadÄ± (.env dosyasÄ±nda HUGGINGFACE_API_KEY)."

            model = config.model or "openai/whisper-large-v3"
            api_url = f"https://router.huggingface.co/hf-inference/models/{model}"
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "audio/flac" # Genelde flac veya wav gÃ¶nderiyoruz, dosya tipine gÃ¶re deÄŸiÅŸebilir ama binary stream Ã¶nemli
            }

            with open(full_path, "rb") as f:
                data = f.read()

            response = requests.post(api_url, headers=headers, data=data)
            
            if response.status_code != 200:
                return f"âŒ Hugging Face API HatasÄ± ({response.status_code}): {response.text}"
            
            result = response.json()
            if "text" in result:
                logger.info(f"Audio transcription completed (HF: {model})")
                return f"ğŸ¤ Transkript ({model}):\n\n{result['text']}"
            else:
                return f"âŒ Beklenmeyen yanÄ±t: {result}"

        except Exception as e:
            logger.error(f"Hugging Face transcription failed: {e}")
            if model_manager.switch_to_fallback("audio"):
                 return transcribe_audio(audio_path)
            return f"âŒ Transkript hatasÄ±: {e}"
            
    else:
         return f"âŒ Desteklenmeyen audio provider: {provider}"


@tool
def text_to_speech(text: str, output_file: str = "speech.mp3") -> str:
    """
    Metni sese Ã§evirir (Text-to-Speech).
    
    Args:
        text: Sese Ã§evrilecek metin
        output_file: Ã‡Ä±ktÄ± dosyasÄ± adÄ±
    
    Returns:
        OluÅŸturulan ses dosyasÄ±nÄ±n yolu
    """
    logger.info(f"Converting text to speech: {text[:50]}...")
    
    output_path = os.path.join(WORKSPACE_DIR, output_file)
    
    # TTS config al
    config = model_manager.get_config("tts")
    provider = config.provider
    
    # OpenAI TTS
    if provider == "openai":
        try:
            import openai
            
            api_key = get_api_key("openai")
            if not api_key:
                 return "âŒ OpenAI API key bulunamadÄ±."

            client = openai.OpenAI(api_key=api_key)
            
            response = client.audio.speech.create(
                model=config.model, # tts-1 or tts-1-hd
                voice="alloy",
                input=text
            )
            
            response.stream_to_file(output_path)
            
            logger.info(f"TTS completed: {output_file}")
            return f"ğŸ”Š Ses dosyasÄ± oluÅŸturuldu: {output_file}"
            
        except Exception as e:
            logger.warning(f"OpenAI TTS failed: {e}")
            if model_manager.switch_to_fallback("tts"):
                 return text_to_speech(text, output_file)
            return f"âŒ TTS hatasÄ±: {e}"
    
    # Google TTS (gTTS) - local veya google provider
    elif provider == "google" or provider == "local":
        try:
            from gtts import gTTS
            
            tts = gTTS(text=text, lang='tr')
            tts.save(output_path)
            
            logger.info(f"TTS completed (gTTS): {output_file}")
            return f"ğŸ”Š Ses dosyasÄ± oluÅŸturuldu: {output_file}"
            
        except ImportError:
            return """âŒ Text-to-Speech iÃ§in gerekli paketler yÃ¼klÃ¼ deÄŸil.

SeÃ§enekler:
1. OpenAI TTS (yÃ¼ksek kalite):
   - :model tts openai tts-1

2. Google TTS (Ã¼cretsiz):
   - pip install gTTS"""
        except Exception as e:
            logger.error(f"TTS failed: {e}")
            return f"âŒ TTS hatasÄ±: {e}"

    # Hugging Face Inference API
    elif provider == "huggingface":
        try:
            import requests
            
            api_key = get_api_key("huggingface")
            if not api_key:
                 return "âŒ Hugging Face API key bulunamadÄ± (.env dosyasÄ±nda HUGGINGFACE_API_KEY)."

            model = config.model or "facebook/mms-tts-eng" # VarsayÄ±lan model
            api_url = f"https://router.huggingface.co/hf-inference/models/{model}"
            headers = {"Authorization": f"Bearer {api_key}"}
            
            payload = {"inputs": text}
            response = requests.post(api_url, headers=headers, json=payload)
            
            if response.status_code != 200:
                return f"âŒ Hugging Face API HatasÄ± ({response.status_code}): {response.text}"
            
            # Ses dosyasÄ±nÄ± kaydet
            with open(output_path, "wb") as f:
                f.write(response.content)
            
            logger.info(f"TTS completed (HF: {model}): {output_file}")
            return f"ğŸ”Š Ses dosyasÄ± oluÅŸturuldu ({model}): {output_file}"

        except Exception as e:
            logger.error(f"Hugging Face TTS failed: {e}")
            if model_manager.switch_to_fallback("tts"):
                 return text_to_speech(text, output_file)
            return f"âŒ TTS hatasÄ±: {e}"
            
    else:
         return f"âŒ Desteklenmeyen TTS provider: {provider}"


def check_vision_support() -> dict:
    """Vision desteÄŸini kontrol et (internal)"""
    # ArtÄ±k doÄŸrudan vision rolÃ¼nÃ¼ kontrol ediyoruz
    config = model_manager.get_config("vision")
    if not config:
        return {"supported": False, "reason": "Vision config yok"}
    
    return {"supported": True, "provider": config.provider, "model": config.model}
